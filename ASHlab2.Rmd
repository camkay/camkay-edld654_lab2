---
title: "EDLD654 - Lab 2"
author: "Ashley Miller, Lea Frank, Stephanie Gluck, & Cameron Kay"
date: "10/21/2020"
output:
  html_document:
    theme: spacelab
    toc: yes
    toc_depth: 4
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(magrittr)

options(scipen = 999)

```

### Read in the `train.csv` data. Please feel free to use `sample_frac()` if you find that the data file is too large for your machine.

```{r, data}

data <- rio::import(here::here("data", "train.csv"))

```

## 1. Initial Split

Set a seed and split the data into a training set and a testing set as two named objects. 

```{r, initial_split}

set.seed(42)

data_split <- initial_split(data)

data_train <- training(data_split)
  
data_test  <- testing(data_split)

```

## 2. Resample

Set a seed and use 10-fold cross-validation to resample the traning data.

```{r, resample}

set.seed(42)

data_train_r10 <- vfold_cv(data_train)

```

## 3. Preprocess

Complete the code maze below by filling in the blanks (____) to create a recipe object that includes:
* a formula model with `score` predicted by 4 predictors
* be sure there are no missing data in your predictors (try `step_naomit()`)
* center and scale all numeric predictors
* dummy code all nominal predictors

```{r, preprocess}

lasso4_rec <- 
  recipe(
    # outcome: test scores
    # predictors: gender, ethnicity, grade level, & student eligibility for free/reduced lunch
    formula = score ~ gndr + ethnic_cd + enrl_grd + econ_dsvntg, 
    data = data_train # training data-set
  ) %>%
  step_naomit(everything(), skip = TRUE) %>% # omit missing data in predictors
  step_string2factor(gndr, ethnic_cd, econ_dsvntg) %>%  # convert character strings to factors
  step_dummy(gndr, ethnic_cd, econ_dsvntg) %>% # convert nominal data into dummy variables
  step_normalize(enrl_grd) # center and scale numeric predictor

```

## 4. Parsnip model

Create a `{parsnip}` lasso model where the penalty hyperparameter is set to be tuned.

```{r, lasso}

# specifying the model
mod_lasso <- linear_reg() %>% # linear regression model
  set_engine("glmnet") %>% # setting the engine
  set_mode("regression") %>% # specifying the outcome (regression or classification); redundant w/^
  set_args(penalty = tune(), # hyper-parameter to be tuned
           mixture = 1) # this is what makes it a lasso model (completely L1)

```

## 5. Fit a tuned lasso model

Complete the code maze below to fit a tuned lasso model.

```{r, lasso_fit_1}

# use grid search with predefined (default) values to find optimal tuning parameter values for penalty
lasso_grid <- grid_regular(penalty())

unique(lasso_grid$penalty) # 3 unique values for penalty

lasso4_fit_1 <- tune_grid(
  object = mod_lasso,
  preprocessor = lasso4_rec,
  resamples = data_train_r10,
  grid = lasso_grid, # our data frame of tuning combinations (3 total)
  control = tune::control_resamples(verbose = FALSE, # turning off for knitted doc
                                    save_pred = TRUE)) # saves analysis predictions for each model


```

### Question A
  + How many models were fit to each fold of `lasso4_fit_1`? (Please provide a numeric answer, *and* use code to corroborate your answer.) 
    + Since there are 3 penalty values being tested, 3 models were fitted to each fold.
  
```{r questionA}

lasso4_fit_1 %>%
  collect_metrics(summarize = FALSE) %>%
  filter(id == "Fold01" & .metric == "rmse") %>%
  nrow()

```

  + Use code to list the different values of `penalty()` that were used.

```{r questionA2}

#unique(lasso_grid$penalty)
lasso_grid

```

## 6. Fit another tuned lasso model

Use your code from (5) above to complete the code maze below to fit a second tuned lasso model, using the same `parsnip` model, `recipe`, and re-sampled object you used before.

```{r, lasso_fit_2}

lasso4_fit_2 <- tune_grid(
  object = mod_lasso, # same parsnip model as fit1
  preprocessor = lasso4_rec, # same recipe as fit1
  resamples = data_train_r10, # same re-sampled object as fit1
  control = tune::control_resamples(verbose = FALSE, # turning off for knitted doc
                                    save_pred = TRUE))

```

### Question B

  + How many models were fit to each fold of `lasso4_fit_2`? (Please provide a numeric answer, *and* use code to corroborate your answer.)
    + 10 models were fit to each fold of `lasso4_fit_2`.

```{r questionB}

lasso4_fit_2 %>%
  collect_metrics(summarize = FALSE) %>%
  filter(id == "Fold01" & .metric == "rmse") %>%
  nrow()

```

  + If this is different than the number of models of `lasso4_fit_1`, please explain why.
    + Yes, the number of models ran was different between models. This is because `lasso4_fit_1` specified a data frame with three possible tuning combinations as an argument to `grid` within the `tune_grid()` function. On the other hand, `lasso4_fit_2` used the default setting, which is to test ten possible tuning combinations.

  + Use code to list the different values of `penalty()` that were used for *lasso4_fit_2*.

```{r questionB2}

lasso4_fit_2 %>%
  collect_metrics(summarize = FALSE) %$%
  unique(penalty)

# an alternative approach:
# unique(collect_metrics(lasso4_fit_2, summarize = FALSE)$penalty)

```

## 7. Complete the necessary steps to create and fit a tuned lasso model that has seven or more predictors (use any tuning grid you like). Note that you will need to create a new recipe as well.

```{r, lasso7}

# create new recipe for model with 7 predictors:
lasso7_rec <- 
  recipe(
    # outcome: test scores
    # predictors: gender, ethnicity, grade level, & student eligibility for free/reduced lunch
    # added: school latitude, school longitude, and.... 
    # whether student received special ed services/was enrolled in gen ed < 40% of  time
    formula = score ~ gndr + ethnic_cd + enrl_grd + econ_dsvntg + lat + lon + dist_sped, 
    data = data_train 
  ) %>%
  step_naomit(everything(), skip = TRUE) %>% 
  step_string2factor(gndr, ethnic_cd, econ_dsvntg, dist_sped) %>% 
  step_dummy(gndr, ethnic_cd, econ_dsvntg, dist_sped) %>%  
  step_normalize(enrl_grd, lat, lon) 

# fit multiple lasso model with 7 predictors
lasso7_fit_2 <- tune_grid(
  object = mod_lasso, # specifying same lasso model object where the penalty is set to be tuned
  preprocessor = lasso7_rec, # using new recipe that contains model formula
  resamples = data_train_r10, # our resample set created via cv_splits
  control = tune::control_resamples(verbose = FALSE, # turn off for knitted doc
                                    save_pred = TRUE))
```

## 8. Compare the metrics from the best lasso model with 4 predictors to the best lasso model with 7+ predicors. Which is best?

```{r question*}

lasso4_fit_2 %>%
  show_best(metric = "rmse", n = 1)

lasso7_fit_2 %>%
  show_best(metric = "rmse", n = 1)

```

Our model with 7 predictors is the best fitting model, yielding an rmse of 99.3 (compared to rmse = 100 for the 4 predictor model).

## 9. Fit a tuned elastic net model with the same predictors from (7). 
  
  + Create and apply a regular grid for the elastic net model
  
```{r question9}

enet_params <- parameters(penalty(), mixture())

# use a grid search with predefined values to find the optimal tuning parameter values
# want 10 different values of penalty and 5 for mixture => 50 models per fold = 500 models total!
enet_grid <- grid_regular(enet_params, levels = c(10, 5)) 

# alternative, more efficient approach:
# enet_grid <- grid_regular(parameters(penalty(), mixture()), levels = c(10, 5))

# sanity check:
unique(enet_grid$penalty) #10 unique values for penalty
unique(enet_grid$mixture) #5 unique values for mixture

```
  
  + Create a new `{parsnip}` elastic net model
  
```{r elastic_model}

# specify the new, tuned elastic net model:
mod_enet7 <- linear_reg() %>%
  set_engine("glmnet") %>% 
  set_mode("regression") %>%
  set_args(penalty = tune(), #placeholder for tuned penalty
           mixture = tune()) #placeholder for tuned mixture; combines L1 (lasso) and L2 (ridge)

```
  
  + Use the same recipe from (7) above
  
```{r elestic_rec}
# fit tuned elastic net model with tune_grid()
enet7_fit1 <- tune_grid( # performs grid search for best combo of tuned hyper-parameters
  object = mod_enet7, # specify elastic net model object
  preprocessor = lasso7_rec, # same recipe (model formula) used for lasso 7 predictor model
  resamples = data_train_r10, # same resample set created via cv_splits
  grid = enet_grid, # our data frame of tuning combinations
  control = tune::control_resamples(verbose = FALSE, # turn off for knitted doc
                                    save_pred = TRUE))
```
  
  + Compare the metrics from the elastic net model to the best lasso model from (8). Which would you choose for your final model? What are the best hyperparameters for that model?

```{r comparisons}

lasso7_fit_2 %>%
  show_best(metric = "rmse", n = 1) # best lasso model

lasso7_fit_2 %>%
  show_best(metric = "rmse", n = 1) %$%
  print(mean) # mean rmse for best fitting lasso model

enet7_fit1 %>%
  show_best(metric = "rmse", n = 1) # best enet model

enet7_fit1 %>%
  show_best(metric = "rmse", n = 1) %$%
  print(mean) # mean rmse for best fitting enet model

#############################
#### best hyper-parameter ###
#############################

lasso7_fit_2 %>%
  show_best(metric = "rmse", n = 1) %$%
  print(penalty) # penalty value for best lasso model

```

While the elastic net model improves model fit, the improvement is extremely small (rmse difference is 0.00008) and comes at the expense of model parsimony/computing power. As such, we do not believe the L2 term associated with the use of the elastic net model merits inclusion; hence we would opt for `lasso7_fit_2` as our final model. In terms of the best hyper-parameter for our model, `lasso7_fit_2` used penalty = `0.0000000001785638`. This value is extremely close to zero, so our tuned lasso model is essentially operating as an OLS regression.
